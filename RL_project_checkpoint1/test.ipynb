{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 55.58\tScore: -2.78\n",
      "Episode 200\tAverage Score: 64.18\tScore: 125.03\n",
      "Episode 300\tAverage Score: 77.21\tScore: 89.117\n",
      "Episode 400\tAverage Score: 103.05\tScore: 125.05\n",
      "Episode 500\tAverage Score: 107.18\tScore: 81.655\n",
      "Episode 600\tAverage Score: 100.27\tScore: 124.63\n",
      "Episode 700\tAverage Score: 97.90\tScore: 77.5559\n",
      "Episode 800\tAverage Score: 91.38\tScore: 32.324\n",
      "Episode 900\tAverage Score: 92.53\tScore: 58.105\n",
      "Episode 1000\tAverage Score: 92.04\tScore: 125.32\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
    "import uuid\n",
    "\n",
    "# Neural Network for Q-value approximation\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network and Target Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=5e-4)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(int(1e5), 64, seed)\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.update_every = 4  # How often to update the target network\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0 and len(self.memory) > self.memory.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "            \n",
    "    def act(self, state, eps=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, 1e-3)\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "# Environment setup and training\n",
    "def train_dqn(n_episodes=1000, max_t=500):\n",
    "    env = MetaDriveEnv(config={\n",
    "        \"map\": \"S\",  # Simple map\n",
    "        \"discrete_action\": True,\n",
    "        \"discrete_throttle_dim\": 3,\n",
    "        \"discrete_steering_dim\": 3,\n",
    "        \"horizon\": max_t,\n",
    "        \"traffic_density\": 0.1,\n",
    "        \"log_level\": 50\n",
    "    })\n",
    "    \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size, seed=0)\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, agent.epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tScore: {score:.2f}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scores = train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
