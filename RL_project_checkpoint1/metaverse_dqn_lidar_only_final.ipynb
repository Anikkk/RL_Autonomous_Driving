{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214f43a6",
   "metadata": {},
   "source": [
    "### DQN using LIDAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20819f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseGhostBodyNode.__del__ at 0x13bac5580>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anikk/Downloads/RL_project/metadrive/metadrive/engine/physics_node.py\", line 52, in __del__\n",
      "    def __del__(self):\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), main_camera: MainCamera(1200, 900), dashboard: DashBoard()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n",
      "\u001b[33;20m[WARNING] Since your screen is too small (1470, 956), we resize the window to (1147, 860). (engine_core.py:234)\u001b[0m\n",
      "2025-04-21 21:08:33.854 Python[88504:22320616] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-21 21:08:33.854 Python[88504:22320616] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 42, Num Scenarios : 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw observation type: <class 'numpy.ndarray'>, shape: (259,)\n",
      "Warning: Observation size 259 doesn’t match expected 240. Truncating to LIDAR data.\n",
      "Raw observation type: <class 'numpy.ndarray'>, shape: (259,)\n",
      "Warning: Observation size 259 doesn’t match expected 240. Truncating to LIDAR data.\n",
      "Raw observation type: <class 'numpy.ndarray'>, shape: (259,)\n",
      "Warning: Observation size 259 doesn’t match expected 240. Truncating to LIDAR data.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "\n",
    "# --- SeedWrapper ---\n",
    "class SeedWrapper(gym.Wrapper):\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- DiscreteActionWrapper ---\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Define a fixed set of discrete actions for MetaDrive\n",
    "        # MetaDrive actions: [steering, throttle, brake]\n",
    "        self.discrete_actions = [\n",
    "            np.array([0.0, 1.0, 0.0]),   # accelerate\n",
    "            np.array([0.0, 0.0, 0.0]),   # no-op\n",
    "            np.array([-0.5, 1.0, 0.0]),  # turn left while accelerating\n",
    "            np.array([0.5, 1.0, 0.0]),   # turn right while accelerating\n",
    "            np.array([0.0, 0.0, 1.0]),   # brake\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "        \n",
    "    def action(self, action_idx):\n",
    "        return self.discrete_actions[action_idx]\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop(\"options\", None)\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- LidarOnlyObservationWrapper ---\n",
    "class LidarOnlyObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, lidar_points=240):\n",
    "        super().__init__(env)\n",
    "        self.lidar_points = lidar_points\n",
    "        # Define the observation space for LIDAR data\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=50.0, shape=(lidar_points,), dtype=np.float32)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Expect obs to be a flat NumPy array\n",
    "        if not isinstance(obs, np.ndarray):\n",
    "            raise ValueError(f\"Observation must be a NumPy array. Got: {type(obs)}\")\n",
    "        \n",
    "        # Debug: Print raw observation shape and type\n",
    "        print(f\"Raw observation type: {type(obs)}, shape: {obs.shape}\")\n",
    "        \n",
    "        # Ensure the data is a 1D array\n",
    "        if obs.ndim > 1:\n",
    "            obs = obs.flatten()\n",
    "        \n",
    "        # Check if the observation size matches expected LIDAR points\n",
    "        if obs.shape[0] != self.lidar_points:\n",
    "            print(f\"Warning: Observation size {obs.shape[0]} doesn’t match expected {self.lidar_points}. Truncating to LIDAR data.\")\n",
    "            obs = obs[:self.lidar_points]  # Assume first 240 elements are LIDAR\n",
    "        \n",
    "        # Convert to float32 array and clip to valid range\n",
    "        lidar_obs = np.array(obs, dtype=np.float32)\n",
    "        lidar_obs = np.clip(lidar_obs, 0.0, 50.0)\n",
    "        return lidar_obs\n",
    "\n",
    "# Example usage with MetaDrive and rendering\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the engine is closed from any previous runs\n",
    "    close_engine()  # Explicitly close any existing engine instance\n",
    "    \n",
    "    # Configure the MetaDrive environment with rendering enabled\n",
    "    config = {\n",
    "        \"use_render\": True,           # Enable 3D rendering for visualization\n",
    "        \"manual_control\": False,      # No human control; agent drives\n",
    "        \"traffic_density\": 0.1,       # Sparse traffic for simplicity\n",
    "        \"num_scenarios\": 1,           # Single scenario\n",
    "        \"start_seed\": 42,             # Fixed seed for reproducibility\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 240, \"distance\": 50.0},  # Enable LIDAR with 240 points\n",
    "            \"side_detector\": {\"num_lasers\": 0},              # Minimize SideDetector output\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0}          # Minimize LaneLineDetector output\n",
    "        },\n",
    "        \"image_observation\": False    # Disable image observations; use LIDAR\n",
    "    }\n",
    "    \n",
    "    # Create the environment and apply wrappers\n",
    "    env = MetaDriveEnv(config)\n",
    "    env = SeedWrapper(env)\n",
    "    env = DiscreteActionWrapper(env)\n",
    "    env = LidarOnlyObservationWrapper(env, lidar_points=240)\n",
    "    \n",
    "    # Check the environment for Gym compatibility\n",
    "    check_env(env)\n",
    "    \n",
    "    # Training loop with visualization\n",
    "    num_episodes = 500  # Number of episodes to train/visualize\n",
    "    max_steps = 200   # Max steps per episode\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()  # Unpack the tuple\n",
    "        print(f\"Episode {episode + 1} started\")\n",
    "        print(\"Initial observation shape:\", obs.shape)\n",
    "        print(\"Initial observation sample (first 10 points):\", obs[:10])\n",
    "        \n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            action = env.action_space.sample()  # Random policy (replace with RL if needed)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)  # Unpack step output\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Render the environment to visualize training\n",
    "            env.render()  # Displays 3D view in a window\n",
    "            \n",
    "            print(f\"Step {step + 1}: Action: {action}, Reward: {reward}, Total Reward: {total_reward}\")\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                print(f\"Episode {episode + 1} ended early: Terminated={terminated}, Truncated={truncated}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode + 1} completed. Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Clean up\n",
    "    env.close()\n",
    "    print(\"Environment closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27537ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), main_camera: MainCamera(1200, 900), dashboard: DashBoard()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n",
      "\u001b[33;20m[WARNING] Since your screen is too small (1470, 956), we resize the window to (1147, 860). (engine_core.py:234)\u001b[0m\n",
      "2025-04-20 17:01:04.765 Python[88796:20745527] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-20 17:01:04.765 Python[88796:20745527] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 42, Num Scenarios : 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.28\n",
      "Intersection detected: Velocity=0.28, Min Distance=0.00\n",
      "Moving: Velocity=0.28\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.03\n",
      "Intersection detected: Velocity=0.03, Min Distance=0.00\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.31\n",
      "Intersection detected: Velocity=0.31, Min Distance=0.00\n",
      "Moving: Velocity=0.31\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=0.27, Min Distance=0.00\n",
      "Intersection detected: Velocity=0.27, Min Distance=0.00\n",
      "Moving: Velocity=0.27\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.54\n",
      "Intersection detected: Velocity=0.54, Min Distance=0.00\n",
      "Moving: Velocity=0.54\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.81\n",
      "Intersection detected: Velocity=0.81, Min Distance=0.00\n",
      "Moving: Velocity=0.81\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.09\n",
      "Intersection detected: Velocity=1.09, Min Distance=0.00\n",
      "Moving: Velocity=1.09\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.32\n",
      "Intersection detected: Velocity=1.32, Min Distance=0.00\n",
      "Moving: Velocity=1.32\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.59\n",
      "Intersection detected: Velocity=1.59, Min Distance=0.00\n",
      "Moving: Velocity=1.59\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.88\n",
      "Intersection detected: Velocity=1.88, Min Distance=0.00\n",
      "Moving: Velocity=1.88\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.06\n",
      "Intersection detected: Velocity=2.06, Min Distance=0.00\n",
      "Moving: Velocity=2.06\n",
      "Observation space: Box(-0.0, 1.0, (259,), float32)\n",
      "Action space: Discrete(5)\n",
      "Episode 1 started\n",
      "Initial observation shape: (259,)\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.03\n",
      "Intersection detected: Velocity=0.03, Min Distance=0.00\n",
      "Step 1: Action: 1, Reward: 0.9610, Total Reward: 0.9610, Epsilon: 1.000\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.31\n",
      "Intersection detected: Velocity=0.31, Min Distance=0.00\n",
      "Moving: Velocity=0.31\n",
      "Step 2: Action: 3, Reward: 0.9723, Total Reward: 1.9333, Epsilon: 0.999\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.58\n",
      "Intersection detected: Velocity=0.58, Min Distance=0.00\n",
      "Moving: Velocity=0.58\n",
      "Step 3: Action: 3, Reward: 1.2834, Total Reward: 3.2167, Epsilon: 0.999\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.86\n",
      "Intersection detected: Velocity=0.86, Min Distance=0.00\n",
      "Moving: Velocity=0.86\n",
      "Step 4: Action: 3, Reward: 1.2945, Total Reward: 4.5111, Epsilon: 0.998\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.14\n",
      "Intersection detected: Velocity=1.14, Min Distance=0.00\n",
      "Moving: Velocity=1.14\n",
      "Step 5: Action: 3, Reward: 1.3055, Total Reward: 5.8167, Epsilon: 0.998\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=1.08, Min Distance=0.00\n",
      "Intersection detected: Velocity=1.08, Min Distance=0.00\n",
      "Moving: Velocity=1.08\n",
      "Step 6: Brake action applied! Velocity: 1.08\n",
      "Step 6: Action: 4, Reward: 1.2432, Total Reward: 7.0599, Epsilon: 0.997\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.29\n",
      "Intersection detected: Velocity=1.29, Min Distance=0.00\n",
      "Moving: Velocity=1.29\n",
      "Step 7: Action: 2, Reward: 1.0116, Total Reward: 8.0715, Epsilon: 0.997\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.42\n",
      "Intersection detected: Velocity=1.42, Min Distance=0.00\n",
      "Moving: Velocity=1.42\n",
      "Step 8: Action: 3, Reward: 1.0169, Total Reward: 9.0884, Epsilon: 0.996\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.56\n",
      "Intersection detected: Velocity=1.56, Min Distance=0.00\n",
      "Moving: Velocity=1.56\n",
      "Step 9: Action: 2, Reward: 1.0226, Total Reward: 10.1109, Epsilon: 0.996\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.52\n",
      "Intersection detected: Velocity=1.52, Min Distance=0.00\n",
      "Moving: Velocity=1.52\n",
      "Step 10: Action: 1, Reward: 1.0207, Total Reward: 11.1316, Epsilon: 0.995\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.48\n",
      "Intersection detected: Velocity=1.48, Min Distance=0.00\n",
      "Moving: Velocity=1.48\n",
      "Step 11: Action: 1, Reward: 1.3192, Total Reward: 12.4508, Epsilon: 0.995\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.44\n",
      "Intersection detected: Velocity=1.44, Min Distance=0.00\n",
      "Moving: Velocity=1.44\n",
      "Step 12: Action: 1, Reward: 1.3177, Total Reward: 13.7685, Epsilon: 0.994\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.65\n",
      "Intersection detected: Velocity=1.65, Min Distance=0.00\n",
      "Moving: Velocity=1.65\n",
      "Step 13: Action: 2, Reward: 1.0260, Total Reward: 14.7945, Epsilon: 0.994\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.92\n",
      "Intersection detected: Velocity=1.92, Min Distance=0.00\n",
      "Moving: Velocity=1.92\n",
      "Step 14: Action: 0, Reward: 1.0367, Total Reward: 15.8312, Epsilon: 0.993\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.07\n",
      "Intersection detected: Velocity=2.07, Min Distance=0.00\n",
      "Moving: Velocity=2.07\n",
      "Step 15: Action: 3, Reward: 1.0426, Total Reward: 16.8738, Epsilon: 0.993\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.12\n",
      "Intersection detected: Velocity=2.12, Min Distance=0.00\n",
      "Moving: Velocity=2.12\n",
      "Step 16: Action: 2, Reward: 1.0448, Total Reward: 17.9186, Epsilon: 0.992\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=2.08, Min Distance=0.00\n",
      "Intersection detected: Velocity=2.08, Min Distance=0.00\n",
      "Moving: Velocity=2.08\n",
      "Step 17: Brake action applied! Velocity: 2.08\n",
      "Step 17: Action: 4, Reward: 1.2832, Total Reward: 19.2017, Epsilon: 0.992\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.26\n",
      "Intersection detected: Velocity=2.26, Min Distance=0.00\n",
      "Moving: Velocity=2.26\n",
      "Step 18: Action: 2, Reward: 1.0504, Total Reward: 20.2522, Epsilon: 0.991\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.52\n",
      "Intersection detected: Velocity=2.52, Min Distance=0.00\n",
      "Moving: Velocity=2.52\n",
      "Step 19: Action: 0, Reward: 1.0607, Total Reward: 21.3129, Epsilon: 0.991\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.69\n",
      "Intersection detected: Velocity=2.69, Min Distance=0.00\n",
      "Moving: Velocity=2.69\n",
      "Step 20: Action: 2, Reward: 1.0675, Total Reward: 22.3804, Epsilon: 0.990\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.94\n",
      "Intersection detected: Velocity=2.94, Min Distance=0.00\n",
      "Moving: Velocity=2.94\n",
      "Step 21: Action: 0, Reward: 1.0775, Total Reward: 23.4579, Epsilon: 0.990\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.90\n",
      "Intersection detected: Velocity=2.90, Min Distance=0.00\n",
      "Moving: Velocity=2.90\n",
      "Step 22: Action: 1, Reward: 1.3760, Total Reward: 24.8339, Epsilon: 0.989\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.19\n",
      "Intersection detected: Velocity=3.19, Min Distance=0.00\n",
      "Moving: Velocity=3.19\n",
      "Step 23: Action: 0, Reward: 1.3875, Total Reward: 26.2214, Epsilon: 0.989\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.15\n",
      "Intersection detected: Velocity=3.15, Min Distance=0.00\n",
      "Moving: Velocity=3.15\n",
      "Step 24: Action: 1, Reward: 1.3861, Total Reward: 27.6075, Epsilon: 0.988\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.25\n",
      "Intersection detected: Velocity=3.25, Min Distance=0.00\n",
      "Moving: Velocity=3.25\n",
      "Step 25: Action: 3, Reward: 1.0901, Total Reward: 28.6976, Epsilon: 0.988\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.48\n",
      "Intersection detected: Velocity=3.48, Min Distance=0.00\n",
      "Moving: Velocity=3.48\n",
      "Step 26: Action: 3, Reward: 1.3991, Total Reward: 30.0967, Epsilon: 0.987\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=3.40, Min Distance=0.00\n",
      "Intersection detected: Velocity=3.40, Min Distance=0.00\n",
      "Moving: Velocity=3.40\n",
      "Step 27: Brake action applied! Velocity: 3.40\n",
      "Step 27: Action: 4, Reward: 1.3361, Total Reward: 31.4328, Epsilon: 0.987\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.44\n",
      "Intersection detected: Velocity=3.44, Min Distance=0.00\n",
      "Moving: Velocity=3.44\n",
      "Step 28: Action: 2, Reward: 1.0977, Total Reward: 32.5305, Epsilon: 0.986\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.47\n",
      "Intersection detected: Velocity=3.47, Min Distance=0.00\n",
      "Moving: Velocity=3.47\n",
      "Step 29: Action: 3, Reward: 1.0989, Total Reward: 33.6294, Epsilon: 0.986\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=3.43, Min Distance=0.00\n",
      "Intersection detected: Velocity=3.43, Min Distance=0.00\n",
      "Moving: Velocity=3.43\n",
      "Step 30: Brake action applied! Velocity: 3.43\n",
      "Step 30: Action: 4, Reward: 1.3373, Total Reward: 34.9667, Epsilon: 0.985\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.51\n",
      "Intersection detected: Velocity=3.51, Min Distance=0.00\n",
      "Moving: Velocity=3.51\n",
      "Step 31: Action: 2, Reward: 1.1004, Total Reward: 36.0671, Epsilon: 0.985\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=3.46, Min Distance=0.00\n",
      "Intersection detected: Velocity=3.46, Min Distance=0.00\n",
      "Moving: Velocity=3.46\n",
      "Step 32: Brake action applied! Velocity: 3.46\n",
      "Step 32: Action: 4, Reward: 1.3383, Total Reward: 37.4053, Epsilon: 0.984\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=3.42, Min Distance=0.00\n",
      "Intersection detected: Velocity=3.42, Min Distance=0.00\n",
      "Moving: Velocity=3.42\n",
      "Step 33: Brake action applied! Velocity: 3.42\n",
      "Step 33: Action: 4, Reward: 1.6368, Total Reward: 39.0421, Epsilon: 0.984\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.51\n",
      "Intersection detected: Velocity=3.51, Min Distance=0.00\n",
      "Moving: Velocity=3.51\n",
      "Step 34: Action: 2, Reward: 1.1004, Total Reward: 40.1425, Epsilon: 0.983\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.73\n",
      "Intersection detected: Velocity=3.73, Min Distance=0.00\n",
      "Moving: Velocity=3.73\n",
      "Step 35: Action: 2, Reward: 1.4090, Total Reward: 41.5515, Epsilon: 0.983\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.71\n",
      "Intersection detected: Velocity=3.71, Min Distance=0.00\n",
      "Moving: Velocity=3.71\n",
      "Step 36: Action: 3, Reward: 1.1084, Total Reward: 42.6599, Epsilon: 0.982\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.74\n",
      "Intersection detected: Velocity=3.74, Min Distance=0.00\n",
      "Moving: Velocity=3.74\n",
      "Step 37: Action: 3, Reward: 1.4097, Total Reward: 44.0696, Epsilon: 0.982\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.78\n",
      "Intersection detected: Velocity=3.78, Min Distance=0.00\n",
      "Moving: Velocity=3.78\n",
      "Step 38: Action: 2, Reward: 1.1110, Total Reward: 45.1806, Epsilon: 0.981\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.73\n",
      "Intersection detected: Velocity=3.73, Min Distance=0.00\n",
      "Moving: Velocity=3.73\n",
      "Step 39: Action: 1, Reward: 1.1094, Total Reward: 46.2900, Epsilon: 0.981\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.02\n",
      "Intersection detected: Velocity=4.02, Min Distance=0.00\n",
      "Moving: Velocity=4.02\n",
      "Step 40: Action: 0, Reward: 1.4208, Total Reward: 47.7108, Epsilon: 0.980\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.31\n",
      "Intersection detected: Velocity=4.31, Min Distance=0.00\n",
      "Moving: Velocity=4.31\n",
      "Step 41: Action: 0, Reward: 1.4323, Total Reward: 49.1431, Epsilon: 0.980\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.37\n",
      "Intersection detected: Velocity=4.37, Min Distance=0.00\n",
      "Moving: Velocity=4.37\n",
      "Step 42: Action: 3, Reward: 1.1348, Total Reward: 50.2780, Epsilon: 0.979\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.64\n",
      "Intersection detected: Velocity=4.64, Min Distance=0.00\n",
      "Moving: Velocity=4.64\n",
      "Step 43: Action: 0, Reward: 1.1457, Total Reward: 51.4237, Epsilon: 0.979\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.68\n",
      "Intersection detected: Velocity=4.68, Min Distance=0.00\n",
      "Moving: Velocity=4.68\n",
      "Step 44: Action: 2, Reward: 1.1474, Total Reward: 52.5710, Epsilon: 0.978\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.69\n",
      "Intersection detected: Velocity=4.69, Min Distance=0.00\n",
      "Moving: Velocity=4.69\n",
      "Step 45: Action: 3, Reward: 1.1478, Total Reward: 53.7188, Epsilon: 0.978\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.72\n",
      "Intersection detected: Velocity=4.72, Min Distance=0.00\n",
      "Moving: Velocity=4.72\n",
      "Step 46: Action: 2, Reward: 1.1490, Total Reward: 54.8678, Epsilon: 0.977\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.83\n",
      "Intersection detected: Velocity=4.83, Min Distance=0.00\n",
      "Moving: Velocity=4.83\n",
      "Step 47: Action: 2, Reward: 1.4531, Total Reward: 56.3209, Epsilon: 0.977\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.77\n",
      "Intersection detected: Velocity=4.77, Min Distance=0.00\n",
      "Moving: Velocity=4.77\n",
      "Step 48: Action: 1, Reward: 1.1506, Total Reward: 57.4715, Epsilon: 0.976\n",
      "Q-values: [ 0.02791795 -0.06795558 -0.10270573 -0.16222042 -0.2197084 ]\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.05\n",
      "Intersection detected: Velocity=5.05, Min Distance=0.00\n",
      "Moving: Velocity=5.05\n",
      "Step 49: Action: 0, Reward: 1.4621, Total Reward: 58.9336, Epsilon: 0.976\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.10\n",
      "Intersection detected: Velocity=5.10, Min Distance=0.00\n",
      "Moving: Velocity=5.10\n",
      "Step 50: Action: 3, Reward: 1.1638, Total Reward: 60.0974, Epsilon: 0.975\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.37\n",
      "Intersection detected: Velocity=5.37, Min Distance=0.00\n",
      "Moving: Velocity=5.37\n",
      "Step 51: Action: 0, Reward: 1.1748, Total Reward: 61.2721, Epsilon: 0.975\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.40\n",
      "Intersection detected: Velocity=5.40, Min Distance=0.00\n",
      "Moving: Velocity=5.40\n",
      "Step 52: Action: 2, Reward: 1.1762, Total Reward: 62.4483, Epsilon: 0.974\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.41\n",
      "Intersection detected: Velocity=5.41, Min Distance=0.00\n",
      "Moving: Velocity=5.41\n",
      "Step 53: Action: 3, Reward: 1.1765, Total Reward: 63.6248, Epsilon: 0.974\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.43\n",
      "Intersection detected: Velocity=5.43, Min Distance=0.00\n",
      "Moving: Velocity=5.43\n",
      "Step 54: Action: 2, Reward: 1.1773, Total Reward: 64.8021, Epsilon: 0.973\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.71\n",
      "Intersection detected: Velocity=5.71, Min Distance=0.00\n",
      "Moving: Velocity=5.71\n",
      "Step 55: Action: 0, Reward: 1.1885, Total Reward: 65.9906, Epsilon: 0.973\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.68\n",
      "Intersection detected: Velocity=5.68, Min Distance=0.00\n",
      "Moving: Velocity=5.68\n",
      "Step 56: Action: 1, Reward: 1.4870, Total Reward: 67.4776, Epsilon: 0.972\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.71\n",
      "Intersection detected: Velocity=5.71, Min Distance=0.00\n",
      "Moving: Velocity=5.71\n",
      "Step 57: Action: 2, Reward: 1.1885, Total Reward: 68.6661, Epsilon: 0.972\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.80\n",
      "Intersection detected: Velocity=5.80, Min Distance=0.00\n",
      "Moving: Velocity=5.80\n",
      "Step 58: Action: 2, Reward: 1.4918, Total Reward: 70.1580, Epsilon: 0.971\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=5.74, Min Distance=0.00\n",
      "Intersection detected: Velocity=5.74, Min Distance=0.00\n",
      "Moving: Velocity=5.74\n",
      "Step 59: Brake action applied! Velocity: 5.74\n",
      "Step 59: Action: 4, Reward: 1.4296, Total Reward: 71.5876, Epsilon: 0.971\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=5.70, Min Distance=0.00\n",
      "Intersection detected: Velocity=5.70, Min Distance=0.00\n",
      "Moving: Velocity=5.70\n",
      "Step 60: Brake action applied! Velocity: 5.70\n",
      "Step 60: Action: 4, Reward: 1.7281, Total Reward: 73.3157, Epsilon: 0.970\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.99\n",
      "Intersection detected: Velocity=5.99, Min Distance=0.00\n",
      "Moving: Velocity=5.99\n",
      "Step 61: Action: 0, Reward: 1.4996, Total Reward: 74.8153, Epsilon: 0.970\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.95\n",
      "Intersection detected: Velocity=5.95, Min Distance=0.00\n",
      "Moving: Velocity=5.95\n",
      "Step 62: Action: 1, Reward: 1.4981, Total Reward: 76.3134, Epsilon: 0.969\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.24\n",
      "Intersection detected: Velocity=6.24, Min Distance=0.00\n",
      "Moving: Velocity=6.24\n",
      "Step 63: Action: 0, Reward: 1.5096, Total Reward: 77.8231, Epsilon: 0.969\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=6.20, Min Distance=0.00\n",
      "Intersection detected: Velocity=6.20, Min Distance=0.00\n",
      "Moving: Velocity=6.20\n",
      "Step 64: Brake action applied! Velocity: 6.20\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.9293\n",
      "Step 64: Action: 4, Reward: 1.7482, Total Reward: 79.5712, Epsilon: 0.968\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.17\n",
      "Intersection detected: Velocity=6.17, Min Distance=0.00\n",
      "Moving: Velocity=6.17\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.7891\n",
      "Step 65: Action: 1, Reward: 1.5067, Total Reward: 81.0779, Epsilon: 0.968\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.45\n",
      "Intersection detected: Velocity=6.45, Min Distance=0.00\n",
      "Moving: Velocity=6.45\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.7063\n",
      "Step 66: Action: 0, Reward: 1.5182, Total Reward: 82.5961, Epsilon: 0.967\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.42\n",
      "Intersection detected: Velocity=6.42, Min Distance=0.00\n",
      "Moving: Velocity=6.42\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.5995\n",
      "Step 67: Action: 1, Reward: 1.5167, Total Reward: 84.1128, Epsilon: 0.967\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.38\n",
      "Intersection detected: Velocity=6.38, Min Distance=0.00\n",
      "Moving: Velocity=6.38\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.5372\n",
      "Step 68: Action: 1, Reward: 1.5153, Total Reward: 85.6281, Epsilon: 0.966\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=6.35, Min Distance=0.00\n",
      "Intersection detected: Velocity=6.35, Min Distance=0.00\n",
      "Moving: Velocity=6.35\n",
      "Step 69: Brake action applied! Velocity: 6.35\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.4990\n",
      "Step 69: Action: 4, Reward: 1.7538, Total Reward: 87.3819, Epsilon: 0.966\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=6.37\n",
      "Intersection detected: Velocity=6.37, Min Distance=0.00\n",
      "Off-road detected: Penalty applied\n",
      "Moving: Velocity=6.37\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.4304\n",
      "Step 70: Action: 2, Reward: -1.6852, Total Reward: 85.6968, Epsilon: 0.965\n",
      "Episode 1 ended early: Terminated=True, Truncated=False\n",
      "Episode 1 completed. Total Reward: 85.6968\n",
      "Target network updated\n",
      "Episode 2 started\n",
      "Initial observation shape: (259,)\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.29\n",
      "Intersection detected: Velocity=0.29, Min Distance=0.00\n",
      "Moving: Velocity=0.29\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 2.0215\n",
      "Step 1: Action: 0, Reward: 0.9715, Total Reward: 0.9715, Epsilon: 0.965\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.25\n",
      "Intersection detected: Velocity=0.25, Min Distance=0.00\n",
      "Moving: Velocity=0.25\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.8582\n",
      "Step 2: Action: 1, Reward: 1.2700, Total Reward: 2.2415, Epsilon: 0.964\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.54\n",
      "Intersection detected: Velocity=0.54, Min Distance=0.00\n",
      "Moving: Velocity=0.54\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.7710\n",
      "Step 3: Action: 0, Reward: 1.2815, Total Reward: 3.5230, Epsilon: 0.964\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=0.50, Min Distance=0.00\n",
      "Intersection detected: Velocity=0.50, Min Distance=0.00\n",
      "Moving: Velocity=0.50\n",
      "Step 4: Brake action applied! Velocity: 0.50\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.6504\n",
      "Step 4: Action: 4, Reward: 1.5200, Total Reward: 5.0431, Epsilon: 0.963\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=0.46, Min Distance=0.00\n",
      "Intersection detected: Velocity=0.46, Min Distance=0.00\n",
      "Moving: Velocity=0.46\n",
      "Step 5: Brake action applied! Velocity: 0.46\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.5595\n",
      "Step 5: Action: 4, Reward: 1.5186, Total Reward: 6.5617, Epsilon: 0.963\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.75\n",
      "Intersection detected: Velocity=0.75, Min Distance=0.00\n",
      "Moving: Velocity=0.75\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.3599\n",
      "Step 6: Action: 0, Reward: 1.2901, Total Reward: 7.8517, Epsilon: 0.962\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.99\n",
      "Intersection detected: Velocity=0.99, Min Distance=0.00\n",
      "Moving: Velocity=0.99\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.2895\n",
      "Step 7: Action: 2, Reward: 0.9998, Total Reward: 8.8515, Epsilon: 0.962\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.16\n",
      "Intersection detected: Velocity=1.16, Min Distance=0.00\n",
      "Moving: Velocity=1.16\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 1.0544\n",
      "Step 8: Action: 3, Reward: 1.0063, Total Reward: 9.8578, Epsilon: 0.961\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.44\n",
      "Intersection detected: Velocity=1.44, Min Distance=0.00\n",
      "Moving: Velocity=1.44\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.9742\n",
      "Step 9: Action: 0, Reward: 1.0175, Total Reward: 10.8753, Epsilon: 0.961\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=1.40, Min Distance=0.00\n",
      "Intersection detected: Velocity=1.40, Min Distance=0.00\n",
      "Moving: Velocity=1.40\n",
      "Step 10: Brake action applied! Velocity: 1.40\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.8438\n",
      "Step 10: Action: 4, Reward: 1.5560, Total Reward: 12.4312, Epsilon: 0.960\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.61\n",
      "Intersection detected: Velocity=1.61, Min Distance=0.00\n",
      "Moving: Velocity=1.61\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.7128\n",
      "Step 11: Action: 3, Reward: 1.0245, Total Reward: 13.4558, Epsilon: 0.960\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=1.56, Min Distance=0.00\n",
      "Intersection detected: Velocity=1.56, Min Distance=0.00\n",
      "Moving: Velocity=1.56\n",
      "Step 12: Brake action applied! Velocity: 1.56\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.6053\n",
      "Step 12: Action: 4, Reward: 1.2623, Total Reward: 14.7181, Epsilon: 0.959\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.52\n",
      "Intersection detected: Velocity=1.52, Min Distance=0.00\n",
      "Moving: Velocity=1.52\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.4760\n",
      "Step 13: Action: 1, Reward: 1.3208, Total Reward: 16.0389, Epsilon: 0.959\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=1.81\n",
      "Intersection detected: Velocity=1.81, Min Distance=0.00\n",
      "Moving: Velocity=1.81\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2490\n",
      "Step 14: Action: 0, Reward: 1.3323, Total Reward: 17.3712, Epsilon: 0.958\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.09\n",
      "Intersection detected: Velocity=2.09, Min Distance=0.00\n",
      "Moving: Velocity=2.09\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2963\n",
      "Step 15: Action: 0, Reward: 1.3438, Total Reward: 18.7150, Epsilon: 0.958\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.38\n",
      "Intersection detected: Velocity=2.38, Min Distance=0.00\n",
      "Moving: Velocity=2.38\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2304\n",
      "Step 16: Action: 0, Reward: 1.3553, Total Reward: 20.0702, Epsilon: 0.957\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.53\n",
      "Intersection detected: Velocity=2.53, Min Distance=0.00\n",
      "Moving: Velocity=2.53\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1983\n",
      "Step 17: Action: 3, Reward: 1.0610, Total Reward: 21.1313, Epsilon: 0.957\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.77\n",
      "Intersection detected: Velocity=2.77, Min Distance=0.00\n",
      "Moving: Velocity=2.77\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1892\n",
      "Step 18: Action: 3, Reward: 1.3709, Total Reward: 22.5021, Epsilon: 0.956\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.70\n",
      "Intersection detected: Velocity=2.70, Min Distance=0.00\n",
      "Moving: Velocity=2.70\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2073\n",
      "Step 19: Action: 1, Reward: 1.0680, Total Reward: 23.5701, Epsilon: 0.956\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=2.98\n",
      "Intersection detected: Velocity=2.98, Min Distance=0.00\n",
      "Moving: Velocity=2.98\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0672\n",
      "Step 20: Action: 0, Reward: 1.3793, Total Reward: 24.9494, Epsilon: 0.955\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.10\n",
      "Intersection detected: Velocity=3.10, Min Distance=0.00\n",
      "Moving: Velocity=3.10\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2768\n",
      "Step 21: Action: 3, Reward: 1.0839, Total Reward: 26.0334, Epsilon: 0.955\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.33\n",
      "Intersection detected: Velocity=3.33, Min Distance=0.00\n",
      "Moving: Velocity=3.33\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1218\n",
      "Step 22: Action: 3, Reward: 1.3932, Total Reward: 27.4265, Epsilon: 0.954\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.26\n",
      "Intersection detected: Velocity=3.26, Min Distance=0.00\n",
      "Moving: Velocity=3.26\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1440\n",
      "Step 23: Action: 1, Reward: 1.0903, Total Reward: 28.5168, Epsilon: 0.954\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.30\n",
      "Intersection detected: Velocity=3.30, Min Distance=0.00\n",
      "Moving: Velocity=3.30\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.3378\n",
      "Step 24: Action: 2, Reward: 1.0921, Total Reward: 29.6089, Epsilon: 0.953\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.48\n",
      "Intersection detected: Velocity=3.48, Min Distance=0.00\n",
      "Moving: Velocity=3.48\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.3036\n",
      "Step 25: Action: 2, Reward: 1.3993, Total Reward: 31.0083, Epsilon: 0.953\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.70\n",
      "Intersection detected: Velocity=3.70, Min Distance=0.00\n",
      "Moving: Velocity=3.70\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2956\n",
      "Step 26: Action: 0, Reward: 1.1078, Total Reward: 32.1161, Epsilon: 0.952\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.66\n",
      "Intersection detected: Velocity=3.66, Min Distance=0.00\n",
      "Moving: Velocity=3.66\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2543\n",
      "Step 27: Action: 1, Reward: 1.4063, Total Reward: 33.5224, Epsilon: 0.952\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=3.62, Min Distance=0.00\n",
      "Intersection detected: Velocity=3.62, Min Distance=0.00\n",
      "Moving: Velocity=3.62\n",
      "Step 28: Brake action applied! Velocity: 3.62\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.2393\n",
      "Step 28: Action: 4, Reward: 1.6448, Total Reward: 35.1672, Epsilon: 0.951\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.91\n",
      "Intersection detected: Velocity=3.91, Min Distance=0.00\n",
      "Moving: Velocity=3.91\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0626\n",
      "Step 29: Action: 0, Reward: 1.4163, Total Reward: 36.5834, Epsilon: 0.951\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.87\n",
      "Intersection detected: Velocity=3.87, Min Distance=0.00\n",
      "Moving: Velocity=3.87\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0537\n",
      "Step 30: Action: 1, Reward: 1.4148, Total Reward: 37.9983, Epsilon: 0.951\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.94\n",
      "Intersection detected: Velocity=3.94, Min Distance=0.00\n",
      "Moving: Velocity=3.94\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0474\n",
      "Step 31: Action: 3, Reward: 1.1177, Total Reward: 39.1159, Epsilon: 0.950\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.89\n",
      "Intersection detected: Velocity=3.89, Min Distance=0.00\n",
      "Moving: Velocity=3.89\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0490\n",
      "Step 32: Action: 1, Reward: 1.1156, Total Reward: 40.2315, Epsilon: 0.950\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=3.94\n",
      "Intersection detected: Velocity=3.94, Min Distance=0.00\n",
      "Moving: Velocity=3.94\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0481\n",
      "Step 33: Action: 2, Reward: 1.1175, Total Reward: 41.3491, Epsilon: 0.949\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.21\n",
      "Intersection detected: Velocity=4.21, Min Distance=0.00\n",
      "Moving: Velocity=4.21\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0538\n",
      "Step 34: Action: 0, Reward: 1.1284, Total Reward: 42.4775, Epsilon: 0.949\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.26\n",
      "Intersection detected: Velocity=4.26, Min Distance=0.00\n",
      "Moving: Velocity=4.26\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0566\n",
      "Step 35: Action: 3, Reward: 1.1304, Total Reward: 43.6079, Epsilon: 0.948\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.38\n",
      "Intersection detected: Velocity=4.38, Min Distance=0.00\n",
      "Moving: Velocity=4.38\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1826\n",
      "Step 36: Action: 3, Reward: 1.4354, Total Reward: 45.0433, Epsilon: 0.948\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=4.32, Min Distance=0.00\n",
      "Intersection detected: Velocity=4.32, Min Distance=0.00\n",
      "Moving: Velocity=4.32\n",
      "Step 37: Brake action applied! Velocity: 4.32\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0549\n",
      "Step 37: Action: 4, Reward: 1.3728, Total Reward: 46.4161, Epsilon: 0.947\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.60\n",
      "Intersection detected: Velocity=4.60, Min Distance=0.00\n",
      "Moving: Velocity=4.60\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1790\n",
      "Step 38: Action: 0, Reward: 1.4442, Total Reward: 47.8603, Epsilon: 0.947\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.89\n",
      "Intersection detected: Velocity=4.89, Min Distance=0.00\n",
      "Moving: Velocity=4.89\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1854\n",
      "Step 39: Action: 0, Reward: 1.4557, Total Reward: 49.3159, Epsilon: 0.946\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=4.85, Min Distance=0.00\n",
      "Intersection detected: Velocity=4.85, Min Distance=0.00\n",
      "Moving: Velocity=4.85\n",
      "Step 40: Brake action applied! Velocity: 4.85\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1731\n",
      "Step 40: Action: 4, Reward: 1.6942, Total Reward: 51.0101, Epsilon: 0.946\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=4.82, Min Distance=0.00\n",
      "Intersection detected: Velocity=4.82, Min Distance=0.00\n",
      "Moving: Velocity=4.82\n",
      "Step 41: Brake action applied! Velocity: 4.82\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1898\n",
      "Step 41: Action: 4, Reward: 1.6927, Total Reward: 52.7028, Epsilon: 0.945\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.86\n",
      "Intersection detected: Velocity=4.86, Min Distance=0.00\n",
      "Moving: Velocity=4.86\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1978\n",
      "Step 42: Action: 3, Reward: 1.1545, Total Reward: 53.8573, Epsilon: 0.945\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=4.99\n",
      "Intersection detected: Velocity=4.99, Min Distance=0.00\n",
      "Moving: Velocity=4.99\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0418\n",
      "Step 43: Action: 3, Reward: 1.4594, Total Reward: 55.3167, Epsilon: 0.944\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=4.92, Min Distance=0.00\n",
      "Intersection detected: Velocity=4.92, Min Distance=0.00\n",
      "Moving: Velocity=4.92\n",
      "Step 44: Brake action applied! Velocity: 4.92\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1749\n",
      "Step 44: Action: 4, Reward: 1.3969, Total Reward: 56.7136, Epsilon: 0.944\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.21\n",
      "Intersection detected: Velocity=5.21, Min Distance=0.00\n",
      "Moving: Velocity=5.21\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1821\n",
      "Step 45: Action: 0, Reward: 1.4682, Total Reward: 58.1819, Epsilon: 0.943\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.17\n",
      "Intersection detected: Velocity=5.17, Min Distance=0.00\n",
      "Moving: Velocity=5.17\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1751\n",
      "Step 46: Action: 1, Reward: 1.4668, Total Reward: 59.6487, Epsilon: 0.943\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.13\n",
      "Intersection detected: Velocity=5.13, Min Distance=0.00\n",
      "Moving: Velocity=5.13\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1776\n",
      "Step 47: Action: 1, Reward: 1.4653, Total Reward: 61.1140, Epsilon: 0.942\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.17\n",
      "Intersection detected: Velocity=5.17, Min Distance=0.00\n",
      "Moving: Velocity=5.17\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1686\n",
      "Step 48: Action: 3, Reward: 1.1669, Total Reward: 62.2809, Epsilon: 0.942\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.17\n",
      "Intersection detected: Velocity=5.17, Min Distance=0.00\n",
      "Moving: Velocity=5.17\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0278\n",
      "Step 49: Action: 2, Reward: 1.1670, Total Reward: 63.4479, Epsilon: 0.941\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.22\n",
      "Intersection detected: Velocity=5.22, Min Distance=0.00\n",
      "Moving: Velocity=5.22\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0319\n",
      "Step 50: Action: 2, Reward: 1.4689, Total Reward: 64.9168, Epsilon: 0.941\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=5.17, Min Distance=0.00\n",
      "Intersection detected: Velocity=5.17, Min Distance=0.00\n",
      "Moving: Velocity=5.17\n",
      "Step 51: Brake action applied! Velocity: 5.17\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0311\n",
      "Step 51: Action: 4, Reward: 1.4069, Total Reward: 66.3237, Epsilon: 0.940\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.23\n",
      "Intersection detected: Velocity=5.23, Min Distance=0.00\n",
      "Moving: Velocity=5.23\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0320\n",
      "Step 52: Action: 2, Reward: 1.1690, Total Reward: 67.4927, Epsilon: 0.940\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.22\n",
      "Intersection detected: Velocity=5.22, Min Distance=0.00\n",
      "Moving: Velocity=5.22\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1773\n",
      "Step 53: Action: 3, Reward: 1.1688, Total Reward: 68.6615, Epsilon: 0.939\n",
      "Applying action 4: [0. 0. 1.]\n",
      "Intersection Brake Rewarded: Velocity=5.18, Min Distance=0.00\n",
      "Intersection detected: Velocity=5.18, Min Distance=0.00\n",
      "Moving: Velocity=5.18\n",
      "Step 54: Brake action applied! Velocity: 5.18\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0292\n",
      "Step 54: Action: 4, Reward: 1.4073, Total Reward: 70.0688, Epsilon: 0.939\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.15\n",
      "Intersection detected: Velocity=5.15, Min Distance=0.00\n",
      "Moving: Velocity=5.15\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1810\n",
      "Step 55: Action: 1, Reward: 1.4658, Total Reward: 71.5346, Epsilon: 0.938\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.43\n",
      "Intersection detected: Velocity=5.43, Min Distance=0.00\n",
      "Moving: Velocity=5.43\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0287\n",
      "Step 56: Action: 0, Reward: 1.4773, Total Reward: 73.0120, Epsilon: 0.938\n",
      "Applying action 0: [0. 1. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.72\n",
      "Intersection detected: Velocity=5.72, Min Distance=0.00\n",
      "Moving: Velocity=5.72\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1773\n",
      "Step 57: Action: 0, Reward: 1.4888, Total Reward: 74.5008, Epsilon: 0.937\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=5.76\n",
      "Intersection detected: Velocity=5.76, Min Distance=0.00\n",
      "Off-road detected: Penalty applied\n",
      "Moving: Velocity=5.76\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0284\n",
      "Step 58: Action: 2, Reward: -1.7095, Total Reward: 72.7913, Epsilon: 0.937\n",
      "Episode 2 ended early: Terminated=True, Truncated=False\n",
      "Episode 2 completed. Total Reward: 72.7913\n",
      "Episode 3 started\n",
      "Initial observation shape: (259,)\n",
      "Applying action 2: [-0.5  1.   0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.28\n",
      "Intersection detected: Velocity=0.28, Min Distance=0.00\n",
      "Moving: Velocity=0.28\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1748\n",
      "Step 1: Action: 2, Reward: 1.2712, Total Reward: 1.2712, Epsilon: 0.936\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.24\n",
      "Intersection detected: Velocity=0.24, Min Distance=0.00\n",
      "Moving: Velocity=0.24\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.3212\n",
      "Step 2: Action: 1, Reward: 0.9697, Total Reward: 2.2409, Epsilon: 0.936\n",
      "Applying action 3: [0.5 1.  0. ]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.51\n",
      "Intersection detected: Velocity=0.51, Min Distance=0.00\n",
      "Moving: Velocity=0.51\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.1710\n",
      "Step 3: Action: 3, Reward: 0.9802, Total Reward: 3.2211, Epsilon: 0.935\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.46\n",
      "Intersection detected: Velocity=0.46, Min Distance=0.00\n",
      "Moving: Velocity=0.46\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0287\n",
      "Step 4: Action: 1, Reward: 0.9785, Total Reward: 4.1996, Epsilon: 0.935\n",
      "Applying action 1: [0. 0. 0.]\n",
      "Intersection Slow Speed Rewarded: Velocity=0.43\n",
      "Intersection detected: Velocity=0.43, Min Distance=0.00\n",
      "Moving: Velocity=0.43\n",
      "Training - States shape: torch.Size([64, 259])\n",
      "Training - Loss: 0.0312\n",
      "Step 5: Action: 1, Reward: 1.2770, Total Reward: 5.4767, Epsilon: 0.934\n",
      "Applying action 0: [0. 1. 0.]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikk/Downloads/RL_project/metadrive-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# --- SeedWrapper ---\n",
    "class SeedWrapper(gym.Wrapper):\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- DiscreteActionWrapper ---\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([0.0, 1.0, 0.0]),   # accelerate\n",
    "            np.array([0.0, 0.0, 0.0]),   # no-op\n",
    "            np.array([-0.5, 1.0, 0.0]),  # turn left while accelerating\n",
    "            np.array([0.5, 1.0, 0.0]),   # turn right while accelerating\n",
    "            np.array([0.0, 0.0, 1.0]),   # brake\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "        \n",
    "    def action(self, action_idx):\n",
    "        action = self.discrete_actions[action_idx]\n",
    "        print(f\"Applying action {action_idx}: {action}\")\n",
    "        return action\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop(\"options\", None)\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- Custom Reward Wrapper ---\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.max_speed = 20.0\n",
    "        self.lane_reward = 0.4  # Increased to encourage lane-keeping\n",
    "        self.progress_reward = 0.8  # Increased to encourage movement\n",
    "        self.brake_reward = 0.8  # Reduced to balance braking\n",
    "        self.smooth_turn_reward = 0.3  # Increased for smoother driving\n",
    "        self.collision_penalty = -1.5  # Increased penalty\n",
    "        self.off_road_penalty = -1.0  # Increased penalty\n",
    "        self.speed_penalty = -0.5  # Increased penalty\n",
    "        self.intersection_speed_threshold = 0.4 * self.max_speed\n",
    "        self.prev_steering = 0.0\n",
    "        self.prev_velocity = 0.0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        velocity = info.get(\"velocity\", 0.0)\n",
    "        cost = info.get(\"cost\", 0.0)\n",
    "        steering = self.env.discrete_actions[action][0]\n",
    "        \n",
    "        lidar_data = obs[:240]\n",
    "        min_distance = np.min(lidar_data) if len(lidar_data) > 0 else 1.0\n",
    "        \n",
    "        out_of_road = info.get(\"out_of_road\", False)\n",
    "        near_intersection = min_distance < 0.4\n",
    "\n",
    "        lane_bonus = self.lane_reward if not out_of_road else 0.0\n",
    "        progress = (velocity / self.max_speed) * self.progress_reward\n",
    "        \n",
    "        brake_bonus = 0.0\n",
    "        if near_intersection:\n",
    "            if action == 4:\n",
    "                if self.prev_velocity == 0.0:\n",
    "                    brake_bonus = 0.4  # Reduced for stopped braking\n",
    "                else:\n",
    "                    brake_bonus = self.brake_reward\n",
    "                print(f\"Intersection Brake Rewarded: Velocity={velocity:.2f}, Min Distance={min_distance:.2f}\")\n",
    "            elif velocity < self.intersection_speed_threshold and velocity > 0.0:\n",
    "                brake_bonus = self.brake_reward * 0.7\n",
    "                print(f\"Intersection Slow Speed Rewarded: Velocity={velocity:.2f}\")\n",
    "        \n",
    "        steering_diff = abs(steering - self.prev_steering)\n",
    "        smooth_turn = self.smooth_turn_reward if steering_diff < 0.3 else 0.0\n",
    "        self.prev_steering = steering\n",
    "        self.prev_velocity = velocity\n",
    "        \n",
    "        collision_cost = self.collision_penalty if cost > 0 else 0.0\n",
    "        off_road_cost = self.off_road_penalty if out_of_road else 0.0\n",
    "        speed_cost = self.speed_penalty if (velocity > self.intersection_speed_threshold and near_intersection) else 0.0\n",
    "        \n",
    "        custom_reward = (lane_bonus + progress + brake_bonus + smooth_turn + \n",
    "                        collision_cost + off_road_cost + speed_cost)\n",
    "        custom_reward = np.clip(custom_reward, -2.0, 2.0)  # Adjusted clipping range\n",
    "        \n",
    "        if near_intersection:\n",
    "            print(f\"Intersection detected: Velocity={velocity:.2f}, Min Distance={min_distance:.2f}\")\n",
    "        if out_of_road:\n",
    "            print(f\"Off-road detected: Penalty applied\")\n",
    "        if velocity > 0.1:\n",
    "            print(f\"Moving: Velocity={velocity:.2f}\")\n",
    "        \n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "\n",
    "# --- Dueling DQN Network ---\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature(x)\n",
    "        value = self.value(features)\n",
    "        advantage = self.advantage(features)\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Dueling DQN Agent ---\n",
    "class DuelingDQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=0.0003, gamma=0.98, epsilon=1.0, epsilon_min=0.01, epsilon_decay_steps=2000, buffer_size=20000, batch_size=64):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DuelingDQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DuelingDQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        self.step_count = 0\n",
    "        self.consecutive_brakes = 0\n",
    "        self.max_consecutive_brakes = 3\n",
    "        self.epsilon_history = []\n",
    "    \n",
    "    def select_action(self, state, greedy=False):\n",
    "        self.step_count += 1\n",
    "        if greedy:\n",
    "            epsilon = 0.0\n",
    "        else:\n",
    "            epsilon = max(self.epsilon_min, self.epsilon - (self.epsilon - self.epsilon_min) * min(self.step_count, self.epsilon_decay_steps) / self.epsilon_decay_steps)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        \n",
    "        if self.consecutive_brakes >= self.max_consecutive_brakes:\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "            self.consecutive_brakes = 0\n",
    "        elif random.random() < epsilon:\n",
    "            action = random.randrange(self.output_dim)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "                print(f\"Q-values: {q_values.cpu().numpy()[0]}\")\n",
    "            action = q_values.argmax().item()\n",
    "        \n",
    "        if action == 4:\n",
    "            self.consecutive_brakes += 1\n",
    "        else:\n",
    "            self.consecutive_brakes = 0\n",
    "        \n",
    "        return action, epsilon  # Return epsilon for logging\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        print(\"Training - States shape:\", states.shape)\n",
    "        \n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        print(f\"Training - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, filepath=\"dueling_dqn_model.pth\"):\n",
    "        torch.save(self.policy_net.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "if __name__ == \"__main__\":\n",
    "    close_engine()\n",
    "    \n",
    "    # Training config (human rendering)\n",
    "    train_config = {\n",
    "        \"use_render\": True,\n",
    "        \"manual_control\": False,\n",
    "        \"traffic_density\": 0.1,\n",
    "        \"num_scenarios\": 1,\n",
    "        \"start_seed\": 42,\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 240, \"distance\": 50.0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0}\n",
    "        },\n",
    "        \"image_observation\": False\n",
    "    }\n",
    "    \n",
    "    # GIF config (top-down view for rgb_array)\n",
    "    gif_config = {\n",
    "        \"use_render\": False,\n",
    "        \"manual_control\": False,\n",
    "        \"traffic_density\": 0.1,\n",
    "        \"num_scenarios\": 1,\n",
    "        \"start_seed\": 42,\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 240, \"distance\": 50.0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0}\n",
    "        },\n",
    "        \"image_observation\": False,\n",
    "        \"top_down_camera_initial_x\": 0,\n",
    "        \"top_down_camera_initial_y\": 0,\n",
    "        \"top_down_camera_initial_z\": 50\n",
    "    }\n",
    "    \n",
    "    # Training environment\n",
    "    env = MetaDriveEnv(train_config)\n",
    "    env = SeedWrapper(env)\n",
    "    env = DiscreteActionWrapper(env)\n",
    "    env = CustomRewardWrapper(env)\n",
    "    \n",
    "    check_env(env)\n",
    "    print(\"Observation space:\", env.observation_space)\n",
    "    print(\"Action space:\", env.action_space)\n",
    "    \n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    agent = DuelingDQNAgent(input_dim, output_dim)\n",
    "    \n",
    "    num_episodes = 10  # Reduced for testing\n",
    "    max_steps = 200\n",
    "    target_update_freq = 5  # More frequent updates\n",
    "    rewards_per_episode = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(f\"Episode {episode + 1} started\")\n",
    "        print(\"Initial observation shape:\", obs.shape)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action, epsilon = agent.select_action(obs)\n",
    "            epsilon_history.append(epsilon)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if action == 4:\n",
    "                print(f\"Step {step + 1}: Brake action applied! Velocity: {info.get('velocity', 0.0):.2f}\")\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            agent.buffer.push(obs, action, reward, next_obs, done)\n",
    "            \n",
    "            agent.train()\n",
    "            env.render()\n",
    "            \n",
    "            print(f\"Step {step + 1}: Action: {action}, Reward: {reward:.4f}, Total Reward: {total_reward:.4f}, Epsilon: {epsilon:.3f}\")\n",
    "            \n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1} ended early: Terminated={terminated}, Truncated={truncated}\")\n",
    "                break\n",
    "        \n",
    "        rewards_per_episode.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} completed. Total Reward: {total_reward:.4f}\")\n",
    "        \n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target()\n",
    "            print(\"Target network updated\")\n",
    "    \n",
    "    # Save the model\n",
    "    agent.save_model(\"dueling_dqn_model.pth\")\n",
    "    \n",
    "    # Plot rewards and epsilon\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_episodes + 1), rewards_per_episode, label=\"Total Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Reward per Episode\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilon_history, label=\"Epsilon\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(\"Epsilon Decay\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_plots.png\")\n",
    "    plt.show()\n",
    "    print(\"Plots saved as 'training_plots.png'\")\n",
    "    \n",
    "    # Close training environment\n",
    "    env.close()\n",
    "    close_engine()\n",
    "    \n",
    "    # GIF creation environment\n",
    "    env = MetaDriveEnv(gif_config)\n",
    "    env = SeedWrapper(env)\n",
    "    env = DiscreteActionWrapper(env)\n",
    "    env = CustomRewardWrapper(env)\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    frames = []\n",
    "    print(\"Creating GIF for greedy episode...\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action, _ = agent.select_action(obs, greedy=True)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        rgb_array = env.render()\n",
    "        if rgb_array is not None:\n",
    "            frames.append(rgb_array)\n",
    "        else:\n",
    "            print(f\"Warning: Step {step + 1} - Render returned None, frame skipped.\")\n",
    "        \n",
    "        print(f\"Greedy Step {step + 1}: Action: {action}, Reward: {reward:.4f}, Total Reward: {total_reward:.4f}\")\n",
    "        \n",
    "        obs = next_obs\n",
    "        if terminated or truncated:\n",
    "            print(f\"Greedy episode ended: Terminated={terminated}, Truncated={truncated}\")\n",
    "            break\n",
    "    \n",
    "    # Save frames as GIF\n",
    "    if frames:\n",
    "        gif_path = \"greedy_episode.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"GIF saved as '{gif_path}' with {len(frames)} frames\")\n",
    "    else:\n",
    "        print(\"No frames captured, GIF not created.\")\n",
    "    \n",
    "    print(f\"Greedy episode completed. Total Reward: {total_reward:.4f}\")\n",
    "    env.close()\n",
    "    print(\"Environment closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), main_camera: MainCamera(1200, 900), dashboard: DashBoard()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n",
      "\u001b[33;20m[WARNING] Since your screen is too small (1470, 956), we resize the window to (1147, 860). (engine_core.py:234)\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 42, Num Scenarios : 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 1] Steps: 155, Total Reward: 93.62, Epsilon: 0.748\n",
      "[Episode 2] Steps: 73, Total Reward: 21.76, Epsilon: 0.653\n",
      "[Episode 3] Steps: 132, Total Reward: 94.48, Epsilon: 0.513\n",
      "[Episode 4] Steps: 54, Total Reward: 8.70, Epsilon: 0.466\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch import nn, optim\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "\n",
    "# --- SeedWrapper ---\n",
    "class SeedWrapper(gym.Wrapper):\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- DiscreteActionWrapper ---\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([0.0, 1.0, 0.0]),   # accelerate\n",
    "            np.array([0.0, 0.0, 0.0]),   # no-op\n",
    "            np.array([-0.5, 1.0, 0.0]),  # turn left\n",
    "            np.array([0.5, 1.0, 0.0]),   # turn right\n",
    "            np.array([0.0, 0.0, 1.0])    # brake\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "\n",
    "    def action(self, action_idx):\n",
    "        return self.discrete_actions[action_idx]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop(\"options\", None)\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- LidarOnlyObservationWrapper ---\n",
    "class LidarOnlyObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, lidar_points=240):\n",
    "        super().__init__(env)\n",
    "        self.lidar_points = lidar_points\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=50.0, shape=(lidar_points,), dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if not isinstance(obs, np.ndarray):\n",
    "            obs = np.array(obs)\n",
    "        return np.clip(obs[:self.lidar_points], 0.0, 50.0).astype(np.float32)\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.stack(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.stack(next_states),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- DQN Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Epsilon-Greedy Action Selection ---\n",
    "def select_action(state, epsilon, policy_net, action_dim):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return policy_net(state_tensor).argmax().item()\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    close_engine()\n",
    "\n",
    "    # MetaDrive Configuration\n",
    "    config = {\n",
    "        \"use_render\": True,\n",
    "        \"manual_control\": False,\n",
    "        \"traffic_density\": 0.05,\n",
    "        \"num_scenarios\": 1,\n",
    "        \"start_seed\": 42,\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 240, \"distance\": 50.0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0}\n",
    "        },\n",
    "        \"image_observation\": False\n",
    "    }\n",
    "\n",
    "    # Create and wrap environment\n",
    "    env = MetaDriveEnv(config)\n",
    "    env = SeedWrapper(env)\n",
    "    env = DiscreteActionWrapper(env)\n",
    "    env = LidarOnlyObservationWrapper(env, lidar_points=240)\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialize networks, optimizer, replay buffer\n",
    "    policy_net = DQN(obs_dim, action_dim)\n",
    "    target_net = DQN(obs_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "    # Hyperparameters\n",
    "    gamma = 0.99\n",
    "    batch_size = 64\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.05\n",
    "    epsilon_decay = 500\n",
    "    target_update_freq = 100\n",
    "    num_episodes = 500\n",
    "    max_steps = 300\n",
    "    steps_done = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * steps_done / epsilon_decay)\n",
    "            action = select_action(state, epsilon, policy_net, action_dim)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = obs\n",
    "            done = terminated or truncated\n",
    "\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                s_batch, a_batch, r_batch, ns_batch, d_batch = buffer.sample(batch_size)\n",
    "\n",
    "                s_batch = torch.FloatTensor(s_batch)\n",
    "                a_batch = torch.LongTensor(a_batch).unsqueeze(1)\n",
    "                r_batch = torch.FloatTensor(r_batch).unsqueeze(1)\n",
    "                ns_batch = torch.FloatTensor(ns_batch)\n",
    "                d_batch = torch.FloatTensor(d_batch).unsqueeze(1)\n",
    "\n",
    "                q_values = policy_net(s_batch).gather(1, a_batch)\n",
    "                next_q = target_net(ns_batch).max(1)[0].unsqueeze(1)\n",
    "                expected_q = r_batch + gamma * next_q * (1 - d_batch)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, expected_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                print(f\"[Episode {episode + 1}] Steps: {step + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "                break\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training finished and environment closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d05616",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd76435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), main_camera: MainCamera(1200, 900), dashboard: DashBoard()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n",
      "\u001b[33;20m[WARNING] Since your screen is too small (1470, 956), we resize the window to (1147, 860). (engine_core.py:234)\u001b[0m\n",
      "2025-04-21 20:15:31.974 Python[78283:22260908] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-21 20:15:31.974 Python[78283:22260908] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 42, Num Scenarios : 1\u001b[0m\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikk/Downloads/RL_project/metadrive-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "from metadrive.component.sensors.lidar import Lidar\n",
    "from metadrive.utils import clip  # Added import for clip function\n",
    "\n",
    "class SeedWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, fixed_seed=42):\n",
    "        super().__init__(env)\n",
    "        self.fixed_seed = fixed_seed\n",
    "        # Set the random seed for numpy and random to ensure consistent traffic\n",
    "        np.random.seed(self.fixed_seed)\n",
    "        random.seed(self.fixed_seed)\n",
    "\n",
    "    def reset(self, seed=None, options=None, **kwargs):\n",
    "        # Always use the fixed seed for consistent map and traffic\n",
    "        kwargs['seed'] = self.fixed_seed\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([0.0, 1.0, 0.0]), # accelerate\n",
    "            np.array([0.0, 0.0, 0.0]), # no-op\n",
    "            np.array([-0.5, 1.0, 0.0]), # turn left\n",
    "            np.array([0.5, 1.0, 0.0]), # turn right\n",
    "            np.array([0.0, 0.0, 1.0]), # brake\n",
    "            np.array([-0.2, 0.5, 0.0]), # soft left (finer steering)\n",
    "            np.array([0.2, 0.5, 0.0]), # soft right (finer steering)\n",
    "            np.array([0.0, 0.5, 0.0]), # gentle accelerate\n",
    "            np.array([-0.05, 0.8, 0.0]), # very slight left (for lane keeping)\n",
    "            np.array([0.05, 0.8, 0.0]), # very slight right (for lane keeping)\n",
    "            np.array([-0.3, 0.8, 0.0]), # medium left\n",
    "            np.array([0.3, 0.8, 0.0]), # medium right\n",
    "            np.array([-0.2, 0.3, 0.0]), # slow left turn\n",
    "            np.array([0.2, 0.3, 0.0]), # slow right turn\n",
    "            np.array([-0.1, 1.0, 0.0]), # gentle left at full speed\n",
    "            np.array([0.1, 1.0, 0.0]), # gentle right at full speed\n",
    "            np.array([-0.025, 0.8, 0.0]), # micro left (for precise lane keeping)\n",
    "            np.array([0.025, 0.8, 0.0]) # micro right (for precise lane keeping)\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "\n",
    "    def action(self, action_idx):\n",
    "        return self.discrete_actions[action_idx]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop(\"options\", None)\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class EnhancedObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, lidar_points=240):\n",
    "        super().__init__(env)\n",
    "        self.lidar_points = lidar_points\n",
    "        # Extended observation space to include intersection-related features\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-float('inf'), \n",
    "            high=float('inf'), \n",
    "            shape=(lidar_points + 10,), # Added 2 more features for intersection\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        base_env = self.env.unwrapped\n",
    "        vehicle = base_env.agents.get(\"agent0\")\n",
    "        lidar_obs = np.clip(obs[:self.lidar_points], 0.0, 50.0).astype(np.float32)\n",
    "        lane_features = np.zeros(10, dtype=np.float32)\n",
    "        \n",
    "        if vehicle and vehicle.lane and vehicle.navigation:\n",
    "            try:\n",
    "                if vehicle.lane in vehicle.navigation.current_ref_lanes:\n",
    "                    current_lane = vehicle.lane\n",
    "                else:\n",
    "                    current_lane = vehicle.navigation.current_ref_lanes[0]\n",
    "                \n",
    "                _, lateral_dist = current_lane.local_coordinates(vehicle.position)\n",
    "                lane_width = vehicle.navigation.get_current_lane_width()\n",
    "                normalized_lateral = lateral_dist / (lane_width/2)\n",
    "                lane_heading = current_lane.heading_at(vehicle.position)\n",
    "                heading_diff = np.abs(np.mod(vehicle.heading_theta - lane_heading, 2 * np.pi))\n",
    "                heading_diff = min(heading_diff, 2 * np.pi - heading_diff) / np.pi\n",
    "                dist_to_intersection = 50.0\n",
    "                if hasattr(vehicle.navigation, 'next_intersection_distance'):\n",
    "                    dist_to_intersection = vehicle.navigation.next_intersection_distance()\n",
    "                target_lane_index = vehicle.navigation.next_target_lane_index\n",
    "                suggested_steering = 0.0\n",
    "                if target_lane_index is not None and target_lane_index != vehicle.lane_index:\n",
    "                    suggested_steering = 1.0 if target_lane_index > vehicle.lane_index else -1.0\n",
    "                speed_norm = vehicle.speed_km_h / vehicle.max_speed_km_h\n",
    "                \n",
    "                # New intersection-related features\n",
    "                intersection_approach = 1.0 - min(dist_to_intersection / 20.0, 1.0) # Proximity to intersection\n",
    "                target_heading_diff = 0.0\n",
    "                if vehicle.navigation.is_in_intersection() and hasattr(vehicle.navigation, 'next_target_lane'):\n",
    "                    next_lane = vehicle.navigation.next_target_lane\n",
    "                    if next_lane:\n",
    "                        target_heading = next_lane.heading_at(next_lane.start)\n",
    "                        target_heading_diff = np.abs(np.mod(vehicle.heading_theta - target_heading, 2 * np.pi))\n",
    "                        target_heading_diff = min(target_heading_diff, 2 * np.pi - target_heading_diff) / np.pi\n",
    "                \n",
    "                lane_features[0] = normalized_lateral\n",
    "                lane_features[1] = heading_diff\n",
    "                lane_features[2] = min(1.0, dist_to_intersection / 50.0)\n",
    "                lane_features[3] = suggested_steering\n",
    "                lane_features[4] = speed_norm\n",
    "                lane_features[5] = 1.0 if vehicle.navigation.is_in_intersection() else 0.0\n",
    "                lane_features[6] = vehicle.steering / vehicle.max_steering\n",
    "                lane_features[7] = 1.0 if vehicle.navigation.is_on_straight_road() else 0.0\n",
    "                lane_features[8] = intersection_approach\n",
    "                lane_features[9] = target_heading_diff\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        combined_obs = np.concatenate([lidar_obs, lane_features])\n",
    "        return combined_obs\n",
    "\n",
    "class FrameStackWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, num_stack=3):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.repeat(env.observation_space.low[np.newaxis, ...], num_stack, axis=0).flatten(),\n",
    "            high=np.repeat(env.observation_space.high[np.newaxis, ...], num_stack, axis=0).flatten(),\n",
    "            shape=(obs_shape[0] * num_stack,),\n",
    "            dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_observation(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.concatenate(list(self.frames))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000, alpha=0.6, beta=0.4, beta_increment=1e-5):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.max_priority if self.size > 0 else 1.0\n",
    "        if self.size < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "            self.priorities[self.position] = max_priority\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "            self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        if self.size < batch_size:\n",
    "            batch = random.sample(list(self.buffer), self.size)\n",
    "            indices = np.random.choice(self.size, self.size, replace=False)\n",
    "        else:\n",
    "            priorities = self.priorities[:self.size] ** self.alpha\n",
    "            probabilities = priorities / np.sum(priorities)\n",
    "            indices = np.random.choice(self.size, batch_size, replace=False, p=probabilities)\n",
    "            batch = [self.buffer[i] for i in indices]\n",
    "            weights = (self.size * probabilities[indices]) ** (-self.beta)\n",
    "            weights /= np.max(weights)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.stack(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.stack(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            np.array(weights, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "        self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class EnhancedDuelingDDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(EnhancedDuelingDDQN, self).__init__()\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_layer(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "def select_action(state, epsilon, policy_net, action_dim, device):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        policy_net.eval()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = policy_net(state_tensor).argmax().item()\n",
    "        policy_net.train()\n",
    "    return action\n",
    "\n",
    "def custom_reward_function(env, vehicle_id: str):\n",
    "    vehicle = env.agents[vehicle_id]\n",
    "    step_info = dict()\n",
    "    reward = 0.0\n",
    "\n",
    "    # Basic reward predefined\n",
    "    if vehicle.lane in vehicle.navigation.current_ref_lanes:\n",
    "        current_lane = vehicle.lane\n",
    "        positive_road = 1\n",
    "    else:\n",
    "        current_lane = vehicle.navigation.current_ref_lanes[0]\n",
    "        current_road = vehicle.navigation.current_road\n",
    "        positive_road = 1 if not current_road.is_negative_road() else -1\n",
    "\n",
    "    long_last, _ = current_lane.local_coordinates(vehicle.last_position)\n",
    "    long_now, lateral_now = current_lane.local_coordinates(vehicle.position)\n",
    "    lane_width = vehicle.navigation.get_current_lane_width()\n",
    "\n",
    "    lateral_factor = clip(1 - 2 * abs(lateral_now) / lane_width, 0.0, 1.0) if env.config[\"use_lateral_reward\"] else 1.0\n",
    "\n",
    "    reward += env.config[\"driving_reward\"] * (long_now - long_last) * lateral_factor * positive_road\n",
    "    reward += env.config[\"speed_reward\"] * (vehicle.speed_km_h / vehicle.max_speed_km_h) * positive_road\n",
    "\n",
    "    # Extra reward for maintaining in the center of lane for stablity\n",
    "    if abs(lateral_now) < 0.5:\n",
    "        reward += 0.3\n",
    "\n",
    "    # Penalty for high speed (Required in DQN for discrete action space)\n",
    "    if vehicle.speed_km_h > 20:\n",
    "        reward -= 1.0\n",
    "\n",
    "    # Curve reward and penalty\n",
    "    try:\n",
    "        road_type = getattr(vehicle.navigation.current_road, \"road_type\", \"\").lower()\n",
    "        is_intersection = any(key in road_type for key in [\"intersection\", \"roundabout\"])\n",
    "    except Exception:\n",
    "        is_intersection = False\n",
    "\n",
    "    try:\n",
    "        curvature = abs(current_lane.curvature_at(long_now))\n",
    "        if curvature > 0.02:\n",
    "            if vehicle.speed_km_h > 20:\n",
    "                reward -= 0.5\n",
    "            else:\n",
    "                reward += 0.2\n",
    "    except:\n",
    "        curvature = 0.0\n",
    "\n",
    "    if is_intersection:\n",
    "        if vehicle.speed_km_h > 15:\n",
    "            reward -= 1.0\n",
    "        try:\n",
    "            future_pos = vehicle.navigation.current_ref_lanes[0].position(long_now + 2.0, 0)\n",
    "            expected_heading = current_lane.heading_at(long_now + 2.0)\n",
    "            heading_diff = abs(vehicle.heading_theta - expected_heading)\n",
    "            if heading_diff < 20:\n",
    "                reward += 0.3\n",
    "            else:\n",
    "                reward -= 0.5\n",
    "        except:\n",
    "            reward -= 0.1\n",
    "\n",
    "        try:\n",
    "            final_long, _ = current_lane.local_coordinates(vehicle.navigation.checkpoints[-1])\n",
    "            if abs(final_long - long_now) < 5:\n",
    "                reward += 0.4\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Steering smoothness\n",
    "    if hasattr(vehicle, \"steering_change\"):\n",
    "        steering_change = abs(vehicle.steering_change)\n",
    "        if steering_change > 0.3:\n",
    "            reward -= 0.2\n",
    "        else:\n",
    "            reward += 0.1\n",
    "\n",
    "    # Add more penalty for crashing into sidewalk\n",
    "    if vehicle.crash_sidewalk:\n",
    "        reward -= 4.0\n",
    "\n",
    "    # Avoid crashing with other vehicles (Not apply now for not using lidar data)\n",
    "    detected_objects = vehicle.lidar.get_surrounding_objects(vehicle)\n",
    "    front_cars = []\n",
    "    side_lanes_clear = {\"left\": True, \"right\": True}\n",
    "\n",
    "    for obj in detected_objects:\n",
    "        if hasattr(obj, \"position\"):\n",
    "            rel_pos = vehicle.convert_to_local_coordinates(obj.position, vehicle.position)\n",
    "            rel_x, rel_y = rel_pos[0], rel_pos[1]\n",
    "            dist = np.linalg.norm(rel_pos)\n",
    "            if 0 < rel_x < 25 and abs(rel_y) < 2.0:\n",
    "                front_cars.append((obj, dist))\n",
    "            if abs(rel_y) > 2.0 and dist < 20:\n",
    "                if rel_y > 0:\n",
    "                    side_lanes_clear[\"left\"] = False\n",
    "                else:\n",
    "                    side_lanes_clear[\"right\"] = False\n",
    "\n",
    "    front_cars.sort(key=lambda x: x[1])\n",
    "\n",
    "    if front_cars:\n",
    "        if side_lanes_clear[\"left\"] or side_lanes_clear[\"right\"]:\n",
    "            reward += 0.2\n",
    "        else:\n",
    "            reward -= 0.5\n",
    "            if vehicle.speed_km_h > 20:\n",
    "                reward -= 0.5\n",
    "\n",
    "    # Terminal state\n",
    "    if env._is_arrive_destination(vehicle):\n",
    "        reward = +env.config[\"success_reward\"]\n",
    "    elif env._is_out_of_road(vehicle):\n",
    "        reward = -env.config[\"out_of_road_penalty\"]\n",
    "    elif vehicle.crash_vehicle:\n",
    "        reward = -env.config[\"crash_vehicle_penalty\"]\n",
    "    elif vehicle.crash_object:\n",
    "        reward = -env.config[\"crash_object_penalty\"]\n",
    "    elif vehicle.crash_sidewalk:\n",
    "        reward = -env.config[\"crash_sidewalk_penalty\"]\n",
    "\n",
    "    step_info[\"step_reward\"] = reward\n",
    "    step_info[\"route_completion\"] = vehicle.navigation.route_completion\n",
    "    return reward, step_info\n",
    "\n",
    "class LinearLRScheduler:\n",
    "    def __init__(self, optimizer, start_lr, end_lr, decay_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.decay_steps = decay_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        progress = min(1.0, self.current_step / self.decay_steps)\n",
    "        lr = self.start_lr + progress * (self.end_lr - self.start_lr)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "def train_ddqn(env, policy_net, target_net, optimizer, buffer, num_episodes, \n",
    "              batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "              target_update_freq, lr_scheduler, device=\"cpu\"):\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    evaluation_scores = []\n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state, epsilon, policy_net, env.action_space.n, device)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            vehicle = env.unwrapped.agents.get(\"agent0\")\n",
    "            if vehicle:\n",
    "                # Using the new custom reward function\n",
    "                reward, _ = custom_reward_function(env.unwrapped, \"agent0\")\n",
    "            \n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if len(buffer) > batch_size:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = buffer.sample(batch_size)\n",
    "                \n",
    "                states_tensor = torch.FloatTensor(states).to(device)\n",
    "                actions_tensor = torch.LongTensor(actions).to(device)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).to(device)\n",
    "                next_states_tensor = torch.FloatTensor(next_states).to(device)\n",
    "                dones_tensor = torch.FloatTensor(dones).to(device)\n",
    "                weights_tensor = torch.FloatTensor(weights).to(device)\n",
    "                \n",
    "                current_q_values = policy_net(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    next_action = policy_net(next_states_tensor).argmax(dim=1)\n",
    "                    next_q_values = target_net(next_states_tensor).gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "                    target_q_values = rewards_tensor + gamma * next_q_values * (1 - dones_tensor)\n",
    "                \n",
    "                loss = (weights_tensor * F.smooth_l1_loss(current_q_values, target_q_values, reduction='none')).mean()\n",
    "                \n",
    "                td_errors = torch.abs(current_q_values - target_q_values).detach().cpu().numpy()\n",
    "                buffer.update_priorities(indices, td_errors + 1e-5)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            if episode_length % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            torch.save(policy_net.state_dict(), f\"models/ddqn_episode_{episode}.pth\")\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            eval_reward = evaluate_policy(env, policy_net, device)\n",
    "            evaluation_scores.append(eval_reward)\n",
    "            print(f\"Episode {episode}/{num_episodes}, Reward: {episode_reward:.2f}, \"\n",
    "                 f\"Length: {episode_length}, Epsilon: {epsilon:.4f}, Eval Reward: {eval_reward:.2f}\")\n",
    "        \n",
    "        if len(evaluation_scores) > 20 and np.mean(evaluation_scores[-10:]) > 200:\n",
    "            print(\"Early stopping triggered: consistent high performance achieved\")\n",
    "            break\n",
    "    \n",
    "    return episode_rewards, episode_lengths, evaluation_scores\n",
    "\n",
    "def evaluate_policy(env, policy_net, device, num_episodes=5):\n",
    "    policy_net.eval()\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = select_action(state, 0.0, policy_net, env.action_space.n, device)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            vehicle = env.unwrapped.agents.get(\"agent0\")\n",
    "            if vehicle:\n",
    "                # Using the new custom reward function\n",
    "                reward, _ = custom_reward_function(env.unwrapped, \"agent0\")\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        total_reward += episode_reward\n",
    "    policy_net.train()\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"num_scenarios\": 1, # Single map\n",
    "        \"use_render\": True,\n",
    "        \"manual_control\": False,\n",
    "        \"traffic_density\": 0.1,\n",
    "        \"start_seed\": 42, # Fixed seed for consistent map\n",
    "        \"driving_reward\": 1.0,\n",
    "        \"speed_reward\": 0.5,\n",
    "        \"success_reward\": 50.0,\n",
    "        \"out_of_road_penalty\": 50.0,\n",
    "        \"crash_vehicle_penalty\": 50.0,\n",
    "        \"crash_object_penalty\": 50.0,\n",
    "        \"crash_sidewalk_penalty\": 50.0,  # Added penalty for sidewalk crashes\n",
    "        \"use_lateral_reward\": True,\n",
    "        \"random_traffic\": False, # Consistent traffic pattern\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 240, \"distance\": 50.0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0}\n",
    "        },\n",
    "        \"image_observation\": False\n",
    "    }\n",
    "    close_engine()\n",
    "    env = MetaDriveEnv(config)\n",
    "    env = SeedWrapper(env, fixed_seed=42) # Enforce fixed seed for map and traffic\n",
    "    env = EnhancedObservationWrapper(env, lidar_points=240)\n",
    "    env = DiscreteActionWrapper(env)\n",
    "    env = FrameStackWrapper(env, num_stack=3)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    policy_net = EnhancedDuelingDDQN(input_dim, output_dim).to(device)\n",
    "    target_net = EnhancedDuelingDDQN(input_dim, output_dim).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "    lr_scheduler = LinearLRScheduler(optimizer, start_lr=1e-4, end_lr=1e-5, decay_steps=100000)\n",
    "\n",
    "    buffer = ReplayBuffer(capacity=100000)\n",
    "\n",
    "    num_episodes = 1000\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.05\n",
    "    epsilon_decay = 200\n",
    "    target_update_freq = 1000\n",
    "\n",
    "    try:\n",
    "        rewards, lengths, eval_scores = train_ddqn(\n",
    "            env, policy_net, target_net, optimizer, buffer, num_episodes,\n",
    "            batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay,\n",
    "            target_update_freq, lr_scheduler, device\n",
    "        )\n",
    "    finally:\n",
    "        env.close()\n",
    "        close_engine()\n",
    "\n",
    "    torch.save(policy_net.state_dict(), \"models/ddqn_final.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb50870",
   "metadata": {},
   "source": [
    "## RGB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), rgb: RGBCamera(84, 84)]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: offscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[33;20m[WARNING] You have set norm_pixel = False, which means the observation will be uint8 values in [0, 255]. Please make sure you have parsed them later before feeding them to network! (metadrive_env.py:117)\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.component.sensors.rgb_camera import RGBCamera\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(states),\n",
    "            torch.tensor(actions, dtype=torch.long),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.stack(next_states),\n",
    "            torch.tensor(dones, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- CNN Q-network ---\n",
    "class CNN_QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions, input_height=84, input_width=84):\n",
    "        super(CNN_QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate feature map size dynamically\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride + 1\n",
    "        \n",
    "        h = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_height, 8, 4), 4, 2), 3, 1)\n",
    "        w = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_width, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = 64 * h * w\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  # Normalize\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# --- Action Selection ---\n",
    "def select_action(state, epsilon, policy_net, num_actions):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state.unsqueeze(0))\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_observation(obs):\n",
    "    # Handle different observation formats\n",
    "    if isinstance(obs, dict) and 'rgb' in obs:\n",
    "        obs = obs['rgb']\n",
    "    \n",
    "    # Convert to tensor with correct dimension ordering\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        if len(obs.shape) == 3 and obs.shape[-1] == 3:  # (H, W, C) format\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1)\n",
    "        elif len(obs.shape) == 3 and obs.shape[0] == 3:  # (C, H, W) format\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected observation shape: {obs.shape}\")\n",
    "    return obs\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train():\n",
    "    close_engine()\n",
    "\n",
    "    width, height = 84, 84\n",
    "    config = {\n",
    "        \"use_render\": False,  # Enable rendering\n",
    "        \"manual_control\": False,\n",
    "        \"image_observation\": True,\n",
    "        \"norm_pixel\": False,\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0},\n",
    "            \"image_source\": \"rgb\"  # Add this to specify which camera to use\n",
    "        },\n",
    "        \"sensors\": {\n",
    "            \"rgb\": (RGBCamera, width, height)\n",
    "        },\n",
    "        \"traffic_density\": 0.05,\n",
    "        \"num_scenarios\": 1,\n",
    "        \"start_seed\": 42,\n",
    "        \"window_size\": (1200, 800),  # Set window size for rendering\n",
    "        \"show_interface\": True,      # Show driving interface\n",
    "        \"show_fps\": True             # Show FPS counter\n",
    "    }\n",
    "\n",
    "    env = MetaDriveEnv(config)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Debug observation format\n",
    "    print(f\"Observation type: {type(obs)}\")\n",
    "    if isinstance(obs, dict):\n",
    "        print(f\"Observation keys: {obs.keys()}\")\n",
    "        if 'rgb' in obs:\n",
    "            print(f\"RGB shape: {obs['rgb'].shape}\")\n",
    "    elif isinstance(obs, np.ndarray):\n",
    "        print(f\"Observation shape: {obs.shape}\")\n",
    "    \n",
    "    try:\n",
    "        state = preprocess_observation(obs)\n",
    "        print(f\"Processed state shape: {state.shape}\")\n",
    "        input_channels = state.shape[0]  # Usually 3 for RGB\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing initial observation: {e}\")\n",
    "        # Fallback to assuming RGB (3 channels)\n",
    "        input_channels = 3\n",
    "        \n",
    "    num_actions = 5\n",
    "\n",
    "    policy_net = CNN_QNetwork(input_channels, num_actions, height, width)\n",
    "    target_net = CNN_QNetwork(input_channels, num_actions, height, width)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "    buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "    num_episodes = 500\n",
    "    max_steps = 300\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.05\n",
    "    epsilon_decay = 10000\n",
    "    target_update_freq = 1000\n",
    "    steps_done = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        try:\n",
    "            state = preprocess_observation(obs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing observation at episode {episode}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Render the environment (3D view)\n",
    "            env.render()\n",
    "            \n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * steps_done / epsilon_decay)\n",
    "            \n",
    "            try:\n",
    "                action = select_action(state, epsilon, policy_net, num_actions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting action: {e}\")\n",
    "                action = random.randint(0, num_actions - 1)\n",
    "\n",
    "            # Discrete → Continuous action\n",
    "            act = {\n",
    "                0: np.array([0.0, 1.0, 0.0]),  # Forward\n",
    "                1: np.array([0.0, 0.0, 0.0]),  # No action\n",
    "                2: np.array([-0.5, 1.0, 0.0]), # Left turn\n",
    "                3: np.array([0.5, 1.0, 0.0]),  # Right turn\n",
    "                4: np.array([0.0, 0.0, 1.0])   # Brake\n",
    "            }[action]\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(act)\n",
    "            \n",
    "            try:\n",
    "                next_state = preprocess_observation(obs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing next observation: {e}\")\n",
    "                continue\n",
    "                \n",
    "            done = terminated or truncated\n",
    "\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Training\n",
    "            if len(buffer) >= batch_size:\n",
    "                try:\n",
    "                    s_batch, a_batch, r_batch, ns_batch, d_batch = buffer.sample(batch_size)\n",
    "\n",
    "                    q_values = policy_net(s_batch).gather(1, a_batch.unsqueeze(1)).squeeze(1)\n",
    "                    next_actions = policy_net(ns_batch).argmax(1)\n",
    "                    next_q = target_net(ns_batch).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "                    expected_q = r_batch + gamma * next_q * (1 - d_batch)\n",
    "\n",
    "                    loss = nn.MSELoss()(q_values, expected_q.detach())\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during training: {e}\")\n",
    "\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538eb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n",
      "\u001b[38;20m[INFO] MetaDrive version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), rgb: RGBCamera(84, 84), main_camera: MainCamera(1200, 800), dashboard: DashBoard()]\u001b[0m\n",
      "\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n",
      "\u001b[38;20m[INFO] Horizon (Max steps per agent): 1000\u001b[0m\n",
      "\u001b[33;20m[WARNING] You have set norm_pixel = False, which means the observation will be uint8 values in [0, 255]. Please make sure you have parsed them later before feeding them to network! (metadrive_env.py:117)\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.3\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: CocoaGraphicsPipe\u001b[0m\n",
      "2025-05-01 17:51:13.191 Python[81201:33646526] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-01 17:51:13.191 Python[81201:33646526] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 42, Num Scenarios : 1\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'direct.showbase.DConfig' has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 288\u001b[39m\n\u001b[32m    285\u001b[39m     env.close()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    163\u001b[39m obs, _ = env.reset()\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Process initial observation to determine input channels\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m state = \u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m input_channels = state.shape[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Usually 3 for RGB\u001b[39;00m\n\u001b[32m    169\u001b[39m num_actions = \u001b[32m5\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mpreprocess_observation\u001b[39m\u001b[34m(obs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected observation shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obs.to(\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'direct.showbase.DConfig' has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from metadrive.envs import MetaDriveEnv\n",
    "from metadrive.component.sensors.rgb_camera import RGBCamera\n",
    "from metadrive.engine.engine_utils import close_engine\n",
    "\n",
    "# --- PPO Configuration ---\n",
    "class PPOConfig:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.lam = 0.95   # GAE lambda\n",
    "        self.clip_param = 0.2  # PPO clipping parameter\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "        self.lr = 3e-4\n",
    "        self.eps = 1e-5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.ppo_epochs = 10\n",
    "        self.batch_size = 64\n",
    "        self.num_episodes = 500\n",
    "        self.max_steps = 300\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- CNN Actor-Critic Network ---\n",
    "class CNN_ActorCritic(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions, input_height=84, input_width=84):\n",
    "        super(CNN_ActorCritic, self).__init__()\n",
    "        # Shared convolutional backbone\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate feature map size\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - kernel_size) // stride + 1\n",
    "        \n",
    "        # Fixed syntax for size calculation\n",
    "        h = conv2d_size_out(\n",
    "            conv2d_size_out(\n",
    "                conv2d_size_out(input_height, 8, 4),\n",
    "                4, 2\n",
    "            ),\n",
    "            3, 1\n",
    "        )\n",
    "        w = conv2d_size_out(\n",
    "            conv2d_size_out(\n",
    "                conv2d_size_out(input_width, 8, 4),\n",
    "                4, 2\n",
    "            ),\n",
    "            3, 1\n",
    "        )\n",
    "        linear_input_size = 64 * h * w\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        action_probs = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.unsqueeze(0).to(config.device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.forward(state)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_observation(obs):\n",
    "    # Handle dictionary observation\n",
    "    if isinstance(obs, dict):\n",
    "        if 'image' not in obs:\n",
    "            raise ValueError(f\"Observation dictionary does not contain 'image' key: {obs.keys()}\")\n",
    "        obs = obs['image']\n",
    "    elif isinstance(obs, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected observation type: {type(obs)}\")\n",
    "    \n",
    "    # Handle 4D observation by selecting first element in first dimension\n",
    "    if len(obs.shape) == 4:\n",
    "        obs = obs[0]  # Assume batch dimension and take first element\n",
    "    \n",
    "    # Validate and convert to tensor\n",
    "    if len(obs.shape) == 3 and obs.shape[-1] == 3:  # (H, W, C)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1)  # To (C, H, W)\n",
    "    elif len(obs.shape) == 3 and obs.shape[0] == 3:  # (C, H, W)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected observation shape: {obs.shape}\")\n",
    "    \n",
    "    return obs.to(config.device)\n",
    "\n",
    "# --- Generalized Advantage Estimation ---\n",
    "def compute_gae(rewards, values, next_value, dones, gamma, lam):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value_t = next_value\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * next_value_t * (1 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1 - dones[i]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value_t = values[i]\n",
    "    return advantages\n",
    "\n",
    "# --- PPO Training Loop ---\n",
    "def train_ppo():\n",
    "    close_engine()\n",
    "    \n",
    "    # Environment configuration\n",
    "    width, height = 84, 84\n",
    "    env_config = {\n",
    "        \"use_render\": True,\n",
    "        \"manual_control\": False,\n",
    "        \"image_observation\": True,\n",
    "        \"norm_pixel\": False,\n",
    "        \"vehicle_config\": {\n",
    "            \"lidar\": {\"num_lasers\": 0},\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0},\n",
    "            \"image_source\": \"rgb\"\n",
    "        },\n",
    "        \"sensors\": {\n",
    "            \"rgb\": (RGBCamera, width, height)\n",
    "        },\n",
    "        \"traffic_density\": 0.05,\n",
    "        \"num_scenarios\": 1,\n",
    "        \"start_seed\": 42,\n",
    "        \"window_size\": (1200, 800),\n",
    "        \"show_interface\": True,\n",
    "        \"show_fps\": True\n",
    "    }\n",
    "    \n",
    "    env = MetaDriveEnv(env_config)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Process initial observation to determine input channels\n",
    "    state = preprocess_observation(obs)\n",
    "    input_channels = state.shape[0]  # Usually 3 for RGB\n",
    "    \n",
    "    num_actions = 5\n",
    "    global config\n",
    "    config = PPOConfig()\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = CNN_ActorCritic(input_channels, num_actions, height, width).to(config.device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr, eps=config.eps)\n",
    "    \n",
    "    # Data structure for rollouts\n",
    "    Transition = namedtuple('Transition', ['state', 'action', 'log_prob', 'reward', 'value', 'done'])\n",
    "    \n",
    "    for episode in range(config.num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        total_reward = 0\n",
    "        rollouts = []\n",
    "        \n",
    "        for step in range(config.max_steps):\n",
    "            env.render()\n",
    "            \n",
    "            # Select action\n",
    "            action, log_prob, value = model.act(state)\n",
    "            \n",
    "            # Map discrete action to continuous\n",
    "            act = {\n",
    "                0: [0.0, 1.0, 0.0],  # Forward\n",
    "                1: [0.0, 0.0, 0.0],  # Coast\n",
    "                2: [-0.5, 1.0, 0.0], # Left\n",
    "                3: [0.5, 1.0, 0.0],  # Right\n",
    "                4: [0.0, 0.0, 1.0]   # Brake\n",
    "            }[action]\n",
    "            \n",
    "            # Step environment\n",
    "            try:\n",
    "                obs, reward, terminated, truncated, _ = env.step(act)\n",
    "                next_state = preprocess_observation(obs)\n",
    "                done = terminated or truncated\n",
    "            except Exception as e:\n",
    "                print(f\"Error during step: {e}\")\n",
    "                done = True\n",
    "                reward = 0\n",
    "                next_state = state\n",
    "            \n",
    "            # Store transition\n",
    "            rollouts.append(Transition(state, action, log_prob, reward, value, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Process rollouts\n",
    "        if not rollouts:\n",
    "            continue\n",
    "        \n",
    "        states = torch.stack([t.state for t in rollouts])\n",
    "        actions = torch.tensor([t.action for t in rollouts], dtype=torch.long).to(config.device)\n",
    "        old_log_probs = torch.tensor([t.log_prob for t in rollouts], dtype=torch.float32).to(config.device)\n",
    "        rewards = [t.reward for t in rollouts]\n",
    "        values = [t.value for t in rollouts]\n",
    "        dones = [t.done for t in rollouts]\n",
    "        \n",
    "        # Compute GAE\n",
    "        with torch.no_grad():\n",
    "            _, next_value = model(states[-1].unsqueeze(0))\n",
    "            next_value = next_value.item()\n",
    "        \n",
    "        advantages = compute_gae(rewards, values, next_value, dones, config.gamma, config.lam)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(config.device)\n",
    "        returns = advantages + torch.tensor(values, dtype=torch.float32).to(config.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO Update\n",
    "        for _ in range(config.ppo_epochs):\n",
    "            indices = np.arange(len(rollouts))\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, len(rollouts), config.batch_size):\n",
    "                end = start + config.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Forward pass\n",
    "                probs, values = model(batch_states)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Compute ratios\n",
    "                ratios = torch.exp(log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Policy loss\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - config.clip_param, 1 + config.clip_param) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + config.value_loss_coef * value_loss - config.entropy_coef * entropy\n",
    "                \n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{config.num_episodes}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ppo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
